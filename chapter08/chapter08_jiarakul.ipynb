{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第8章: ニューラルネット"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第6章で取り組んだニュース記事のカテゴリ分類を題材として，ニューラルネットワークでカテゴリ分類モデルを実装する．なお，この章ではPyTorch, TensorFlow, Chainerなどの機械学習プラットフォームを活用せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "import gensim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 70. 単語ベクトルの和による特徴量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-05-18 00:57:11--  https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 29224203 (28M) [application/x-httpd-php]\n",
      "Saving to: ‘NewsAggregatorDataset.zip’\n",
      "\n",
      "NewsAggregatorDatas 100%[===================>]  27.87M  12.7MB/s    in 2.2s    \n",
      "\n",
      "2021-05-18 00:57:14 (12.7 MB/s) - ‘NewsAggregatorDataset.zip’ saved [29224203/29224203]\n",
      "\n",
      "Archive:  NewsAggregatorDataset.zip\n",
      "  inflating: 2pageSessions.csv       \n",
      "   creating: __MACOSX/\n",
      "  inflating: __MACOSX/._2pageSessions.csv  \n",
      "  inflating: newsCorpora.csv         \n",
      "  inflating: __MACOSX/._newsCorpora.csv  \n",
      "  inflating: readme.txt              \n",
      "  inflating: __MACOSX/._readme.txt   \n"
     ]
    }
   ],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\n",
    "!unzip NewsAggregatorDataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table(\"newsCorpora.csv\", header=None)\n",
    "df.set_axis(['ID','TITLE','URL','PUBLISHER','CATEGORY','STORY','HOSTNAME','TIMESTAMP'], axis=1, inplace=True)\n",
    "df = df.set_index('ID')\n",
    "\n",
    "extractPublisher = [\"Reuters\", \"Huffington Post\", \"Businessweek\", \"Contactmusic.com\", \"Daily Mail\"]\n",
    "df = df.loc[df['PUBLISHER'].isin(extractPublisher)]\n",
    "\n",
    "df = df.sample(frac=1, random_state=42)\n",
    "train, valid, test = np.split(df, [int(.8*len(df)), int(.9*len(df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_convert(df,filename):\n",
    "  cat_list_num = list(map(lambda x:{'b':0,\n",
    "                               't':1,\n",
    "                               'e':2,\n",
    "                               'm':3}[x], df[\"CATEGORY\"]))\n",
    "  vec = torch.tensor(cat_list_num)\n",
    "  torch.save(vec, filename)\n",
    "  return vec\n",
    "\n",
    "Ytrain = cat_convert(train, \"Ytrain.pt\")\n",
    "Yvalid = cat_convert(valid, \"Yvalid.pt\")\n",
    "Ytest = cat_convert(test, \"Ytest.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin',binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "  text = text.lower()\n",
    "  text = re.sub(r\"[.?!-/:;\\\"]\",'',text)\n",
    "  text = re.sub(r\"[0-9]\",\"\",text)\n",
    "  return text\n",
    "\n",
    "def title_convert(df, filename):\n",
    "  titles = df.TITLE\n",
    "  title_processed = [preprocess(x) for x in titles]\n",
    "  Xmatrix = []\n",
    "  for title in title_processed:\n",
    "      vec_list = [model[word] for word in title.split() if word in model]\n",
    "      result = sum(vec_list)/len(vec_list)\n",
    "      Xmatrix.append(result)\n",
    "  matrix = torch.tensor(Xmatrix)\n",
    "  torch.save(matrix, filename)\n",
    "  return matrix\n",
    "\n",
    "Xtrain = title_convert(train, \"Xtrain.pt\")\n",
    "Xvalid = title_convert(valid, \"Xvalid.pt\")\n",
    "Xtest = title_convert(test, \"Xtest.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 71. 単層ニューラルネットワークによる予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "  def __init__(self, input_size, output_size):\n",
    "    super().__init__()\n",
    "    self.fc = nn.Linear(input_size, output_size, bias=False)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    x = self.fc(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = NeuralNetwork(300, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = Xtrain[0]\n",
    "y1_hat = nn_model(x1)\n",
    "y1_hat = F.softmax(y1_hat, dim=-1)\n",
    "\n",
    "x1_4 = Xtrain[:4]\n",
    "Y_hat = nn_model(x1_4)\n",
    "Y_hat = F.softmax(Y_hat,dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y1:  tensor([0.2633, 0.2466, 0.2557, 0.2344], grad_fn=<SoftmaxBackward>)\n",
      "Y:  tensor([[0.2633, 0.2466, 0.2557, 0.2344],\n",
      "        [0.2604, 0.2365, 0.2546, 0.2484],\n",
      "        [0.2483, 0.2531, 0.2474, 0.2512],\n",
      "        [0.2508, 0.2534, 0.2564, 0.2394]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(\"y1: \", y1_hat)\n",
    "print(\"Y: \", Y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 72. 損失と勾配の計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1\n",
      "loss:  0.4615009129047394\n",
      "gradient:  tensor([[ 0.0133,  0.0129, -0.0017,  ...,  0.0031, -0.0183,  0.0122],\n",
      "        [-0.0024, -0.0023,  0.0003,  ..., -0.0006,  0.0033, -0.0022],\n",
      "        [-0.0098, -0.0095,  0.0013,  ..., -0.0023,  0.0135, -0.0090],\n",
      "        [-0.0011, -0.0011,  0.0001,  ..., -0.0003,  0.0015, -0.0010]])\n",
      "\n",
      "x1_4\n",
      "loss:  0.45604491233825684\n",
      "gradient:  tensor([[ 0.0020, -0.0092,  0.0025,  ..., -0.0071, -0.0126,  0.0058],\n",
      "        [-0.0007, -0.0016, -0.0048,  ..., -0.0058,  0.0038, -0.0056],\n",
      "        [-0.0011,  0.0109,  0.0026,  ...,  0.0133,  0.0081,  0.0005],\n",
      "        [-0.0003, -0.0002, -0.0003,  ..., -0.0003,  0.0006, -0.0006]])\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "print('x1')\n",
    "# nn_model.zero_grad()\n",
    "loss_val = loss(nn_model(Xtrain[:1]), Ytrain[:1])\n",
    "nn_model.zero_grad()\n",
    "loss_val.backward()\n",
    "print('loss: ',loss_val.item())\n",
    "print('gradient: ', nn_model.fc.weight.grad)\n",
    "\n",
    "print()\n",
    "\n",
    "print('x1_4')\n",
    "nn_model.zero_grad()\n",
    "loss_val = loss(nn_model(Xtrain[:4]), Ytrain[:4])\n",
    "loss_val.backward()\n",
    "print('loss: ',loss_val.item())\n",
    "print('gradient: ', nn_model.fc.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 73. 確率的勾配降下法による学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Loss:  0.7469321696685154\n",
      "1 Loss:  0.5347062539660854\n",
      "2 Loss:  0.4720564242519347\n",
      "3 Loss:  0.4388999477232451\n",
      "4 Loss:  0.41780008454485124\n",
      "5 Loss:  0.4028401354253966\n",
      "6 Loss:  0.3917135834041966\n",
      "7 Loss:  0.38267476251028226\n",
      "8 Loss:  0.3753227742265804\n",
      "9 Loss:  0.36899962025507455\n",
      "10 Loss:  0.3637679615466244\n",
      "11 Loss:  0.35915568479385634\n",
      "12 Loss:  0.3549647993638016\n",
      "13 Loss:  0.3513511230579701\n",
      "14 Loss:  0.348077141320929\n",
      "15 Loss:  0.34505089412233986\n",
      "16 Loss:  0.3423283442078807\n",
      "17 Loss:  0.3398545049823668\n",
      "18 Loss:  0.33747984060287545\n",
      "19 Loss:  0.3353467882944791\n",
      "20 Loss:  0.33338885565168175\n",
      "21 Loss:  0.3314033477682825\n",
      "22 Loss:  0.32965787524133694\n",
      "23 Loss:  0.3280970786674602\n",
      "24 Loss:  0.32649239637683675\n",
      "25 Loss:  0.32511538716395244\n",
      "26 Loss:  0.3236670662238755\n",
      "27 Loss:  0.3222549095174427\n",
      "28 Loss:  0.3209206235403993\n",
      "29 Loss:  0.3198561786389379\n",
      "30 Loss:  0.3187309418407338\n",
      "31 Loss:  0.31770051930515547\n",
      "32 Loss:  0.3166330592297352\n",
      "33 Loss:  0.3155823742898499\n",
      "34 Loss:  0.3146150185964926\n",
      "35 Loss:  0.31385294665625657\n",
      "36 Loss:  0.3128783774285113\n",
      "37 Loss:  0.3120936216452594\n",
      "38 Loss:  0.31124599227988775\n",
      "39 Loss:  0.31043290240000637\n",
      "40 Loss:  0.3097204490914796\n",
      "41 Loss:  0.30902745171386997\n",
      "42 Loss:  0.30820131189158423\n",
      "43 Loss:  0.3076983762696657\n",
      "44 Loss:  0.3068803647665064\n",
      "45 Loss:  0.3063461867383298\n",
      "46 Loss:  0.3057277816899701\n",
      "47 Loss:  0.3051454337164298\n",
      "48 Loss:  0.30457578136848956\n",
      "49 Loss:  0.30392710736689915\n",
      "50 Loss:  0.30343466780677153\n",
      "51 Loss:  0.30289939673344735\n",
      "52 Loss:  0.3023498794088387\n",
      "53 Loss:  0.30176197362757384\n",
      "54 Loss:  0.30151009938011186\n",
      "55 Loss:  0.3008888382792866\n",
      "56 Loss:  0.30045238972465527\n",
      "57 Loss:  0.2999518451360483\n",
      "58 Loss:  0.2996094249377903\n",
      "59 Loss:  0.2991649010440994\n",
      "60 Loss:  0.29865194290608726\n",
      "61 Loss:  0.29831677372173276\n",
      "62 Loss:  0.29790782015388756\n",
      "63 Loss:  0.2975555798396264\n",
      "64 Loss:  0.2971521849818395\n",
      "65 Loss:  0.29681192399763867\n",
      "66 Loss:  0.29651187362059434\n",
      "67 Loss:  0.29602748229434134\n",
      "68 Loss:  0.29576403201890933\n",
      "69 Loss:  0.2953791135341081\n",
      "70 Loss:  0.2950165031715229\n",
      "71 Loss:  0.29465979305029394\n",
      "72 Loss:  0.29441207577332296\n",
      "73 Loss:  0.29412914217873415\n",
      "74 Loss:  0.29385539047029036\n",
      "75 Loss:  0.2934984920537676\n",
      "76 Loss:  0.2932018936208244\n",
      "77 Loss:  0.2930495009772012\n",
      "78 Loss:  0.29261197901388214\n",
      "79 Loss:  0.2922120583911381\n",
      "80 Loss:  0.2922177207641324\n",
      "81 Loss:  0.2918051576776666\n",
      "82 Loss:  0.2915966583581417\n",
      "83 Loss:  0.29130006557143295\n",
      "84 Loss:  0.29104494355271304\n",
      "85 Loss:  0.29072869083509567\n",
      "86 Loss:  0.29056424557252597\n",
      "87 Loss:  0.29038158045593476\n",
      "88 Loss:  0.2900957184548735\n",
      "89 Loss:  0.29000378031917434\n",
      "90 Loss:  0.289698544289803\n",
      "91 Loss:  0.2895037966285705\n",
      "92 Loss:  0.28924479205874193\n",
      "93 Loss:  0.28898420168154776\n",
      "94 Loss:  0.28885038696160853\n",
      "95 Loss:  0.28863991665733546\n",
      "96 Loss:  0.28847657927247017\n",
      "97 Loss:  0.28826150246419413\n",
      "98 Loss:  0.287969260133706\n",
      "99 Loss:  0.2879439235149199\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "optimizer = optim.SGD(nn_model.parameters(), lr=learning_rate)\n",
    "dataset = TensorDataset(Xtrain, Ytrain)\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "  total_loss = 0\n",
    "  for x, y in loader:\n",
    "    optimizer.zero_grad()\n",
    "    loss_value = loss(nn_model(x),y)\n",
    "    loss_value.backward()\n",
    "    optimizer.step()\n",
    "    total_loss += loss_value.item()\n",
    "  print(epoch, \"Loss: \", total_loss/len(loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 74. 正解率の計測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: tensor([0.9036])\n",
      "test accuracy: tensor([0.8913])\n"
     ]
    }
   ],
   "source": [
    "def acc(loader, model):\n",
    "  sum_acc = 0\n",
    "  for x, y in loader: \n",
    "    y_pred = nn_model(x)\n",
    "    sum_acc += y_pred.argmax(1) == y\n",
    "  return sum_acc/len(loader)\n",
    "\n",
    "print('train accuracy: ',end='')\n",
    "dataset = TensorDataset(Xtrain, Ytrain)\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "print(acc(loader, model))\n",
    "\n",
    "print('test accuracy: ',end='')\n",
    "dataset = TensorDataset(Xtest, Ytest)\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "print(acc(loader, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 75. 損失と正解率のプロット"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 76. チェックポイント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 77. ミニバッチ化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 78. GPU上での学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 79. 多層ニューラルネットワーク"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
